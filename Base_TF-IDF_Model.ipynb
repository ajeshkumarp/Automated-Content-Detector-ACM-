{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import platform\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, KFold\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score,roc_curve,auc, accuracy_score, f1_score,precision_score,hamming_loss\n",
    "import json\n",
    "import os\n",
    "#import SupportFunctions as sf\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "import time as tm\n",
    "import gc\n",
    "import psutil\n",
    "from collections import OrderedDict\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ï»¿Text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\" \\n\\n == Before adding a new product to the l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             ï»¿Text  toxic  severe_toxic  \\\n",
       "0  Thank you for understanding. I think very high...      0             0   \n",
       "1                   :Dear god this site is horrible.      0             0   \n",
       "2  \"::: Somebody will invariably try to add Relig...      0             0   \n",
       "3  \" \\n\\n It says it right there that it IS a typ...      0             0   \n",
       "4  \" \\n\\n == Before adding a new product to the l...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \n",
       "0        0       0       0              0  \n",
       "1        0       0       0              0  \n",
       "2        0       0       0              0  \n",
       "3        0       0       0              0  \n",
       "4        0       0       0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('train.csv',sep=',', encoding='ISO-8859-1')\n",
    "test_data  = pd.read_csv('multiLabelTest.csv',sep=',', encoding='ISO-8859-1')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(temp_ds,str_idx):\n",
    "    x=temp_ds.iloc[:,idx:].sum()\n",
    "    rowsums=temp_ds.iloc[:,idx:].sum(axis=1)\n",
    "    temp_ds['clean']=(rowsums==0)\n",
    "    \n",
    "    df_majority = temp_ds[temp_ds.clean==True]\n",
    "    df_minority = temp_ds[temp_ds.clean==False]\n",
    "\n",
    "    df_majority = df_majority.sample(frac = 1)\n",
    "    df_majority_downsampled = df_majority.head(40000)\n",
    "\n",
    "    temp_ds = pd.concat([df_majority_downsampled,df_minority])\n",
    "    temp_ds = temp_ds.drop('clean', axis=1)\n",
    "    return temp_ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.shape(train_data),np.shape(test_data))\n",
    "\n",
    "#train_data = clean_data(train_data,2)\n",
    "#test_data = clean_data(test_data,1)\n",
    "\n",
    "#print(np.shape(train_data),np.shape(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_data.iloc[:,2:]\n",
    "train_ds = pd.DataFrame()\n",
    "train_ds['comment_text'] = train_data.iloc[:,1]\n",
    "\n",
    "\n",
    "test_y = test_data.iloc[:,1:]\n",
    "test_ds = pd.DataFrame()\n",
    "test_ds['comment_text'] = test_data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic ratio is 10.0\n",
      "severe_toxic ratio is 1.0\n",
      "obscene ratio is 5.0\n",
      "threat ratio is 0.0\n",
      "insult ratio is 5.0\n",
      "identity_hate ratio is 1.0\n",
      "\n",
      "\n",
      "toxic ratio is 10.0\n",
      "severe_toxic ratio is 1.0\n",
      "obscene ratio is 6.0\n",
      "threat ratio is 0.0\n",
      "insult ratio is 5.0\n",
      "identity_hate ratio is 1.0\n"
     ]
    }
   ],
   "source": [
    "for idx in train_y.columns:\n",
    "    print(\"{} ratio is {}\".format(idx,np.round(len(train_y[train_y[idx]==1])*100/len(train_y)),4))\n",
    "print(\"\\n\")\n",
    "for idx in test_y.columns:\n",
    "    print(\"{} ratio is {}\".format(idx,np.round(len(test_y[test_y[idx]==1])*100/len(test_y)),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#clean data\n",
    "\n",
    "retainNumbers = True\n",
    "useStemming = True\n",
    "useLemma = False\n",
    "use_hashing = True\n",
    "\n",
    "train_ds = train_ds.fillna(\"-\")\n",
    "test_ds = test_ds.fillna(\"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text, lemmatizer, stemmer, retainNumbers):\n",
    "    import re\n",
    "    from nltk import WordNetLemmatizer\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    import sys\n",
    "    import pandas as pd\n",
    "    # good overall paper on what stemmers and lemmatizer do\n",
    "    # http://www.kenbenoit.net/courses/tcd2014qta/readings/Jivani_ijcta2011020632.pdf\n",
    "\n",
    "    if pd.isnull(text) == True:\n",
    "        return \"\"\n",
    "\n",
    "    if retainNumbers == True:\n",
    "        #remove any character not in the listed range\n",
    "        text = re.sub(\"[^0-9a-zA-Z\\.]+\", \" \", text)\n",
    "    else:\n",
    "        text = re.sub(\"[^a-zA-Z]+\", \" \", text)\n",
    "\n",
    "    #remove extra whitespace\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    text = text.lower()\n",
    "\n",
    "    #split text\n",
    "\n",
    "    # http://www.nltk.org/api/nltk.stem.html#nltk.stem.wordnet.WordNetLemmatizer\n",
    "    if(lemmatizer == True):\n",
    "\n",
    "        try:\n",
    "            wordnet_lemmatizer = WordNetLemmatizer()\n",
    "            newText = \"\"\n",
    "            for word in text.split():\n",
    "                newText = \" \".join((newText, wordnet_lemmatizer.lemmatize(word)))\n",
    "            text = newText.strip(\" \")\n",
    "            #text = wordnet_lemmatizer.lemmatize(text)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"--- downloading nltk wordnet corpora\")\n",
    "            import nltk\n",
    "            nltk.download('wordnet')\n",
    "            newText = \"\"\n",
    "\n",
    "            for word in text.split():\n",
    "                newText = \" \".join((newText, wordnet_lemmatizer.lemmatize(word)))\n",
    "            text = newText.strip(\" \")\n",
    "\n",
    "    #http://www.nltk.org/howto/stem.html\n",
    "    if (stemmer == True):\n",
    "        try:\n",
    "            stm = SnowballStemmer(\"english\")\n",
    "            newText = \"\"\n",
    "            for word in text.split():\n",
    "                newText = \" \".join((newText, stm.stem(word)))\n",
    "            text = newText.strip(\" \")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"--- downloading nltk snowball data\")\n",
    "            import nltk\n",
    "            nltk.download('snowball_data')\n",
    "            newText = \"\"\n",
    "            for word in text.split():\n",
    "                newText = \" \".join((newText, stm.stem(word)))\n",
    "            text = newText.strip(\" \")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds['comment_text'] = train_ds['comment_text'].apply(preprocessor, args=(useLemma, useStemming, retainNumbers))\n",
    "test_ds['comment_text'] = test_ds['comment_text'].apply(preprocessor, args=(useLemma, useStemming, retainNumbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_ds['comment_text'].values.ravel()\n",
    "test_data = test_ds['comment_text'].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using TF-IDF\n",
      "vocab length :- 5000\n",
      "(159571, 5000)\n",
      "(63978, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(\"using TF-IDF\")\n",
    "\n",
    "minDocFreq = 1\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True,sublinear_tf=True, analyzer=u'word',stop_words='english',min_df=minDocFreq,ngram_range=(1, 1), max_features=5000)\n",
    "train_data = vectorizer.fit_transform(train_data)\n",
    "vocab = vectorizer.vocabulary_\n",
    "print(\"vocab length :- \" + str(len(vocab)))\n",
    "\n",
    "test_data = vectorizer.transform(test_data)\n",
    "\n",
    "print(np.shape(train_data))\n",
    "print(np.shape(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=300)\n",
    "svd.fit(train_data)\n",
    "train_data = svd.transform(train_data)\n",
    "test_data = svd.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nfold_Cross_Valid(X, y, clf):\n",
    "    scores=[]\n",
    "    scores_f1= []\n",
    "    scores_hamming = []\n",
    "\n",
    "    #X =np.array(X)\n",
    "    y =np.array(y)\n",
    "    #ss = StratifiedShuffleSplit(n_splits=2, test_size=0.3, random_state=21)\n",
    "    #ss = KFold(len(y), n_folds=3,shuffle=True,indices=None)\n",
    "    #ss = KFold(n_splits=3, random_state=21, shuffle=True)\n",
    "    \n",
    "    train_size=0.5\n",
    "    #ss = IterativeStratification(n_splits=2, order=1)  \n",
    "    ss = IterativeStratification(\n",
    "        n_splits=3, order=1)#, sample_distribution_per_fold=[1.0-train_size, train_size, ])\n",
    "    i = 1\n",
    "\n",
    "    for trainCV, testCV in ss.split(X, y):\n",
    "        X_train, X_test= X[trainCV], X[testCV]\n",
    "        y_train, y_test= y[trainCV], y[testCV]\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        #y_pred=clf.predict_proba(X_test)[:,1]\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        scores.append(accuracy_score(y_test,y_pred))\n",
    "        print(\" %d-iteration...Accuracy %s \" % (i,scores))\n",
    "        \n",
    "        #scores_f1.append(f1_score(y_true=y_test,y_pred=y_pred,average='weighted'))\n",
    "        #print(\" %d-iteration...F1 %s \" % (i,scores_f1))\n",
    "        \n",
    "        scores_hamming.append(hamming_loss(y_true=y_test,y_pred=y_pred))\n",
    "        print(\" %d-iteration...Hamming %s \" % (i,scores_hamming))\n",
    "        \n",
    "        hamming_loss\n",
    "        \n",
    "        i = i + 1\n",
    "\n",
    "    #Average ROC from cross validation\n",
    "    scores=np.array(scores)\n",
    "    print (\"Acc CV Score:\",np.mean(scores))\n",
    "    #print (\"F1 CV Score:\",np.mean(np.array(scores_f1)))\n",
    "    print (\"Hamming CV Score:\",np.mean(np.array(scores_hamming)))\n",
    "\n",
    "    print(\"***************Ending Kfold Cross validation***************\")\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1-iteration...Accuracy [0.9085069716434279] \n",
      " 1-iteration...Hamming [0.025834247219175936] \n",
      " 2-iteration...Accuracy [0.9085069716434279, 0.9107316308945637] \n",
      " 2-iteration...Hamming [0.025834247219175936, 0.02602224659251136] \n",
      " 3-iteration...Accuracy [0.9085069716434279, 0.9107316308945637, 0.904558984803384] \n",
      " 3-iteration...Hamming [0.025834247219175936, 0.02602224659251136, 0.026570578098073006] \n",
      " 4-iteration...Accuracy [0.9085069716434279, 0.9107316308945637, 0.904558984803384, 0.9103757325060324] \n",
      " 4-iteration...Hamming [0.025834247219175936, 0.02602224659251136, 0.026570578098073006, 0.02534131386251345] \n",
      " 5-iteration...Accuracy [0.9085069716434279, 0.9107316308945637, 0.904558984803384, 0.9103757325060324, 0.9063136456211812] \n",
      " 5-iteration...Hamming [0.025834247219175936, 0.02602224659251136, 0.026570578098073006, 0.02534131386251345, 0.02698313227844796] \n",
      "Acc CV Score: 0.9080973930937178\n",
      "Hamming CV Score: 0.026150303610144345\n",
      "***************Ending Kfold Cross validation***************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BinaryRelevance(classifier=RandomForestClassifier(max_depth=10, n_estimators=30,\n",
       "                                                  n_jobs=-1),\n",
       "                require_dense=[True, True])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RF training\n",
    "clf = BinaryRelevance(RandomForestClassifier(n_jobs=-1, n_estimators=30,max_depth = 10))\n",
    "Nfold_score = Nfold_Cross_Valid(train_data, train_y, clf)\n",
    "clf.fit(train_data, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9072649973428366\n",
      "0.028376942073837882\n"
     ]
    }
   ],
   "source": [
    "#RF prediction\n",
    "pred_y = clf.predict(test_data)\n",
    "print(accuracy_score(test_y,pred_y))\n",
    "print(hamming_loss(test_y,pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
